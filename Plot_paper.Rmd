---
  title: "Plots for the paper"
author: "Lorenzo Ghilotti"
date: "`r Sys.Date()`"
output: 
  pdf_document:
  fig_width: 6
fig_height: 4
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, echo = TRUE)
```


```{r}
library(ProductFormFA)
```

In this section, we illustrate the performance of the different proposed models under different scenarios. Through these simulations, we aim at highlighting the model-specific properties that we have discussed in the previous sections, both in terms of in-sample rarefaction curve and in terms of prediction of the number of unseen features in future samples. To this end, we distinguish between two broad classes of generating mechanisms, i.e. (i) bounded-features scenarios, where the number of features observable in the population is bounded, that is $\exists K^* > 0$ such that $\lim_{n \to \infty} K_n = K^*$ almost surely; (ii) unbounded-features scenarios, where the number of features observable in the population is unbounded, that is $\lim_{n \to \infty} K_n = \infty$ almost surely. We are going to discuss how the two examples of feature allocation models - *Mixtures of IBP* and *Mixtures of Beta-Bernoulli with $N$ features* - are suitable for different scenarios. Remark: it is not possible to establish with certainty whether the true generating mechanism belongs to a bounded-features scenario or an unbounded-features one by looking at the data. This decision pertains to the analyst, based on expert knowledge of the problem, translated into proper modeling assumptions. Through the following simulations, we just want to show that, if the true generating mechanism were known, then some models are more suitable than others to fit the data. As a consequence, when the knowledge of the problem suggests one of the two scenarios, some models might be preferred with respect to others.  

## Bounded-features scenarios
In this section, we consider 5 ecological species detection models, as in \href{https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13979}{Chiu (2022)}. Within these settings, the individuals are geographical sites where species of animals are collected (each species is a feature which might be displayed by the individual). In each scenario, the total number of species is $H=500$ and the species occurrence probabilities $(\pi_1,\ldots,\pi_H)$ are determined. We compare the Gamma mixture of IBP, the mixture of Beta-Bernoulli with Poisson prior on $N$ and the mixture of Beta-Bernoulli with negative binomial prior on $N$. For each setting and each model we show the following quantities, estimated for different dimensions of the training set: (i.a) the in-sample rarefaction curve (on a single dataset), (i.b) the extrapolated rarefaction curve on the test set (on a single dataset), (ii) the accuracy of the estimated number of unseen features in the test sample, over $D=50$ replicated datasets. Specifically, we focus on the following measure of Accuracy, denoted with $\nu_m^{(n)}$, 
\[
  \nu_m^{(n)} := \frac{1}{1 + \frac{|\tilde{K}_m^{(n)} - \hat{K}_m^{(n)} |}{\tilde{K}_n}},
  \]
where $\tilde{K}_m^{(n)}$ is the observed number of unseen features in the test set, $\hat{K}_m^{(n)}$ is the expected value of the statistic $K_m^{(n)}$ and $\tilde{K}_n$ is the observed number of features in the training set. Note that $\nu_m^{(n)} \in (0,1]$, with $\nu_m^{(n)} = 1$ meaning perfect estimation.

Moreover, for the mixtures of Beta-Bernoulli, we also report (iii) the posterior distribution of the total number of features (on a single dataset), (iv) the expected value of the posterior distribution of the total number of features, over $D=50$ replicated datasets.

\newpage

### Scenario 1: the homogeneous model

```{r, include=FALSE, eval=TRUE}
####
##### MODEL 1: the homogeneous model ############################
####

rm(list=ls())
library(ggmcmc)
library(coda)
library(ggpubr)
library(scales)
library(tidyverse)
library(gridExtra)
library(grid)
library(patchwork)

##### Single dataset -> Ntilde (Poiss/NB) and extrapolation (Poiss/NB/Gamma) #####

###### 1) Read results:  MCMC convergence ####################
load(file = "chao_model_simulation/m1/m1_params_poiss.Rda")
load(file =  "chao_model_simulation/m1/m1_params_negbin.Rda")
load(file =  "chao_model_simulation/m1/m1_params_ibp.Rda" )
load(file =  "chao_model_simulation/m1/m1_params_sp.Rda" )

###### 2) Read results: samples from limiting distributions (Poiss/NB) ##############
load(file = "chao_model_simulation/m1/m1_ntilde_poiss.Rda")
load(file = "chao_model_simulation/m1/m1_ntilde_negbin.Rda")

###### 3) Read results: CI for extrapolation (Poiss/NB/Gamma) ################
list_kmn_pred_test_poiss <- readRDS(file = "chao_model_simulation/m1/m1_ci_poiss.rds")
list_kmn_pred_test_negbin <- readRDS(file = "chao_model_simulation/m1/m1_ci_negbin.rds")
list_kmn_pred_test_ibp <- readRDS(file = "chao_model_simulation/m1/m1_ci_ibp.rds")
list_kmn_pred_test_sp <- readRDS(file = "chao_model_simulation/m1/m1_ci_sp.rds")

###### 3.b) Read results: CI for insample (Poiss/NB/Gamma) ################
list_kn_rarefaction_poiss <- readRDS(file = "chao_model_simulation/m1/m1_ci_insample_poiss.rds")
list_kn_rarefaction_negbin <- readRDS(file = "chao_model_simulation/m1/m1_ci_insample_negbin.rds")
list_kn_rarefaction_ibp <- readRDS(file = "chao_model_simulation/m1/m1_ci_insample_ibp.rds")
list_kn_rarefaction_sp <- readRDS(file = "chao_model_simulation/m1/m1_ci_insample_sp.rds")


###### 4) Read the data ###############################
data_mat <- readRDS(file = "chao_model_simulation/m1/m1_data_mat.rds")
data_list <- create_features_list(data_mat)
L <- nrow(data_mat)
Ms <- sapply(list_kmn_pred_test_poiss, function(l) length(l$medians))
Ns <- L - Ms


##### Accuracy on multiple datasets #####

###### 1) Read results: limit distribution estimates #####
load(file = "chao_model_simulation/m1/m1_avg_ntilde_poiss.Rda")
load(file = "chao_model_simulation/m1/m1_avg_ntilde_negbin.Rda")

###### 2) Read results: quantities on accuracy #####
load(file = "chao_model_simulation/m1/m1_obs_train.Rda")
load(file = "chao_model_simulation/m1/m1_obs_new.Rda")

load(file = "chao_model_simulation/m1/m1_est_new_poiss.Rda")
load(file = "chao_model_simulation/m1/m1_est_new_negbin.Rda")
load(file = "chao_model_simulation/m1/m1_est_new_ibp.Rda")
load(file = "chao_model_simulation/m1/m1_est_new_sp.Rda")

D <- nrow(avg_ntilde_poiss)

```

Set $\pi_k = 0.05$, for $k =1,\ldots,H$. Let $L$ be the total dimension of the dataset, and consider different dimensions for the training set $n$, i.e. 

```{r, echo=FALSE}
# Total dimension of the dataset
cat("L = ",L, "\n", sep = "\t")
# Different dimensions of the training set
cat("n = ",as.numeric(Ns), "\n", sep = "\t")
```

Here, the curve representing the number of observed features in increasing samples, where the grey vertical lines indicate the different training dimensions.

```{r, echo=FALSE, fig.align = "center"}

plot_trajectory(data_list) + 
  ggtitle("Number of observed features in increasing samples") +
  geom_vline(xintercept = Ns, color="grey", linetype="dashed", linewidth=1)

```
\newpage
Here, we report (i.a) the in-sample rarefaction curve (on a single dataset).

```{r, echo=FALSE}
gg_kn_rarefaction_all  <- vector(mode="list", length = length(Ns))

for (j in 1:length(Ns)){
  N <- Ns[j]
  
  train_mat <- data_mat[1:N,]
  # convert the binary matrix into list of features
  train_list <- create_features_list(train_mat)
  
  gg_kn_rarefaction_all[[j]] <- plot_Kn_median_and_rarefaction_all(
    train_list = train_list,
    ci_poiss = list_kn_rarefaction_poiss[[paste0("N.",N)]], 
    ci_negbin = list_kn_rarefaction_negbin[[paste0("N.",N)]],
    ci_ibp = list_kn_rarefaction_ibp[[paste0("N.",N)]],
    n_avg = 100) +
    ggtitle(paste0("n = ", N)) + theme(plot.title = element_text(size=12)) 
    
  
  if (j != 1){
    gg_kn_rarefaction_all[[j]] <- gg_kn_rarefaction_all[[j]] +
      theme(#axis.text.y = element_blank(),
            #axis.ticks.y = element_blank(),
            axis.title.y = element_blank() )
  }
  
}
```

```{r, echo=FALSE, fig.align = "center"}
# Print plots
fig_m1_rare_ <- wrap_plots(gg_kn_rarefaction_all, nrow = 1, ncol = 4) +   plot_layout(guides = "collect") & theme(legend.position = 'right') & xlab(NULL) & theme(plot.margin = margin(5.5, 5.5, 0, 5.5))

fig_m1_rare <- wrap_elements(panel = fig_m1_rare_) +
  labs(tag = "# observations") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom") 

ggsave(filename = "Plots_paper/plot_m1_rare.png", width = 10, height = 4, dpi = 300, units = "in", device='png')
```

Here, we report (i.b) the extrapolated rarefaction curve (on a single dataset).

```{r, echo=FALSE}
gg_kmn_pred_test_all  <- vector(mode="list", length = length(Ns))

for (j in 1:length(Ns)){
  N <- Ns[j]
  M <- L - N
  
  train_mat <- data_mat[1:N,]
  test_mat <- data_mat[(N+1):L, ]
  # convert the binary matrix into list of features
  train_list <- create_features_list(train_mat)
  test_list <- create_features_list(test_mat)
  
  
  gg_kmn_pred_test_all[[j]] <- plot_Kmn_median_pred_and_test_all(
    train_list = train_list,
    test_list = test_list,
    ci_poiss = list_kmn_pred_test_poiss[[paste0("N.",N)]], 
    ci_negbin = list_kmn_pred_test_negbin[[paste0("N.",N)]],
    ci_ibp = list_kmn_pred_test_ibp[[paste0("N.",N)]],
    n_avg = 100) +
    ggtitle(paste0("n = ", N)) + theme(plot.title = element_text(size=12))

  
  if (j != 1){
    gg_kmn_pred_test_all[[j]] <- gg_kmn_pred_test_all[[j]] +
      theme(#axis.text.y = element_blank(),
            #axis.ticks.y = element_blank(),
            axis.title.y = element_blank() )
  }
  
}

```


```{r, echo=FALSE, fig.align = "center"}
# Print plots
fig_m1_pred_ <- wrap_plots(gg_kmn_pred_test_all, nrow = 1, ncol = 4) +   plot_layout(guides = "collect") & theme(legend.position = 'right') & xlab(NULL) & theme(plot.margin = margin(5.5, 5.5, 0, 5.5))

fig_m1_pred <- wrap_elements(panel = fig_m1_pred_) +
  labs(tag = "# observations") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom") 

ggsave(filename = "Plots_paper/plot_m1_pred.png", width = 10, height = 4, dpi = 300, units = "in", device='png')

```

Here, we report (ii) the accuracy of the estimated number of unseen features in the test sample, over $D=50$ replicated datasets. 

```{r, echo=FALSE, fig.align = "center"}
# Print plots
acc_alt_poiss <- compute_accuracy(obs_new, est_new_poiss, obs_train)

acc_alt_negbin <- compute_accuracy(obs_new, est_new_negbin, obs_train)

acc_alt_ibp <- compute_accuracy(obs_new, est_new_ibp, obs_train)

#acc_alt_sp <- compute_accuracy(obs_new, est_new_sp, obs_train)


acc_alt_poiss_long <- gather(acc_alt_poiss, training, Accuracy, 
                             paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "BBmixP", N = rep(Ns, each = D))

acc_alt_negbin_long <- gather(acc_alt_negbin, training , Accuracy, 
                              paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "BBmixNB", N = rep(Ns, each = D))

acc_alt_ibp_long <- gather(acc_alt_ibp, training , Accuracy, 
                           paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "3IBPmix", N = rep(Ns, each = D))

#acc_alt_sp_long <- gather(acc_alt_sp, training , Accuracy,                           paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%  add_column(Model = "SB-SP", N = rep(Ns, each = D))

joint_alt_long <- bind_rows(acc_alt_poiss_long, acc_alt_negbin_long, 
                            acc_alt_ibp_long) %>%
    mutate(Model = fct_relevel(Model, c("3IBPmix", "BBmixP", "BBmixNB"))) 


# plots
ggplot(joint_alt_long, aes( y=Accuracy, x=Model, fill=Model)) + 
  geom_boxplot() + 
  facet_wrap(~N, labeller = labeller(N = ~ paste("n = ", .x)), scales = "free", nrow = 1) +
  theme_bw() + 
  theme(strip.text.x = element_text(size = 12))+
  theme(#panel.grid.major = element_blank(),
        #panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = pretty_breaks()) +
  rremove("xlab") +
  scale_fill_manual(values = c("3IBPmix" = "orangered1" ,
                                  "BBmixP" = "forestgreen",
                                  "BBmixNB" = "royalblue1")) 

ggsave(filename = "Plots_paper/plot_m1_accuracy.png", width = 10, height = 4, dpi = 300, units = "in", device='png')

```

\newpage
For the mixtures of Beta-Bernoulli, we report (iii) the posterior distribution of the total number of features (on a single dataset).


```{r, echo=FALSE, fig.align = "center"}


gg_ntilde_poiss_long <- gather(gg_ntilde_poiss, training, estimate, 
                               paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), 
                               factor_key=TRUE) %>%
  add_column(Model= "BBmixP", N = rep(Ns, each = nrow(gg_ntilde_poiss))) %>% 
  filter(estimate < 2000)


gg_ntilde_negbin_long <- gather(gg_ntilde_negbin, training, estimate, 
                                paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), 
                                factor_key=TRUE) %>%
  add_column(Model= "BBmixNB", N = rep(Ns, each = nrow(gg_ntilde_negbin))) %>% 
  filter(estimate < 2000)


joint_total_long <- bind_rows(gg_ntilde_poiss_long, gg_ntilde_negbin_long) %>%
    mutate(Model = fct_relevel(Model, c("BBmixP", "BBmixNB"))) 

# plot
ggplot(joint_total_long, aes(x = estimate, color = Model)) + 
  #geom_density(alpha = 0.5, linewidth = 0.8) +
  stat_density(aes(x=estimate, colour=Model),
                     geom="line",position="identity") +
  geom_vline(xintercept = 500, color="black", linetype="dashed", linewidth=0.8) +
  facet_wrap(~N, labeller = labeller(N = ~ paste("n = ", .x)), scales = "free", nrow = 1) +
  theme_bw() + 
  theme(strip.text.x = element_text(size = 12))+
  theme(#panel.grid.major = element_blank(),
        #panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_y_continuous(breaks = pretty_breaks()) +
  xlab("# distinct features") + rremove("ylab") +
  scale_color_manual(values = c("BBmixP" = "forestgreen",
                                  "BBmixNB" = "royalblue1")) 

ggsave(filename = "Plots_paper/plot_m1_richness.png", width = 10, height = 4, dpi = 300, units = "in", device='png')


```


Finally, we report (iv) the expected value of the posterior distribution of the total number of features, over $D=50$ replicated datasets.

```{r, echo=FALSE, fig.align = "center"}
# Plot limit distribution estimate
avg_ntilde_poiss_long <- gather(avg_ntilde_poiss, training, estimate, 
                                paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model = "BBmixP", N = rep(Ns[1:length(Ns)], each = D))

avg_ntilde_negbin_long <- gather(avg_ntilde_negbin, training, estimate, 
                                 paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model = "BBmixNB", N = rep(Ns[1:length(Ns)], each = D))

joint_ntilde_long <- bind_rows(avg_ntilde_poiss_long, avg_ntilde_negbin_long) %>%
      mutate(Model = fct_relevel(Model, c("BBmixP", "BBmixNB"))) 


# plots
ggplot(joint_ntilde_long, aes( y=estimate, x=Model, fill=Model)) + 
  geom_boxplot() + 
  geom_hline(yintercept = 500,color="black", linetype="dashed", linewidth=0.8) +
  facet_wrap(~N, labeller = labeller(N = ~ paste("n = ", .x)), scales = "free", nrow = 1) +
  theme_bw() + 
  theme(strip.text.x = element_text(size = 12))+
  theme(#panel.grid.major = element_blank(),
        #panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = pretty_breaks()) +
  rremove("xlab") + ylab("# distinct features") +
  scale_fill_manual(values = c("BBmixP" = "forestgreen",
                                  "BBmixNB" = "royalblue1")) 

ggsave(filename = "Plots_paper/plot_m1_rep_rich.png", width = 10, height = 4, dpi = 300, units = "in", device='png')



```


#######################################

\newpage
### Scenario 2: the random uniform model

```{r, include=FALSE, eval=TRUE}
####
##### MODEL 2: the random uniform model ############################
####

rm(list=ls())
library(ggmcmc)
library(coda)
library(ggpubr)
library(scales)
library(tidyverse)
library(gridExtra)
library(grid)

##### Single dataset -> Ntilde (Poiss/NB) and extrapolation (Poiss/NB/Gamma) #####

###### 1) Read results:  MCMC convergence ####################
load(file = "chao_model_simulation/m2/m2_params_poiss.Rda")
load(file =  "chao_model_simulation/m2/m2_params_negbin.Rda")
load(file =  "chao_model_simulation/m2/m2_params_ibp.Rda" )
load(file =  "chao_model_simulation/m2/m2_params_sp.Rda" )

###### 2) Read results: samples from limiting distributions (Poiss/NB) ##############
load(file = "chao_model_simulation/m2/m2_ntilde_poiss.Rda")
load(file = "chao_model_simulation/m2/m2_ntilde_negbin.Rda")

###### 3) Read results: CI for extrapolation (Poiss/NB/Gamma) ################
list_kmn_pred_test_poiss <- readRDS(file = "chao_model_simulation/m2/m2_ci_poiss.rds")
list_kmn_pred_test_negbin <- readRDS(file = "chao_model_simulation/m2/m2_ci_negbin.rds")
list_kmn_pred_test_ibp <- readRDS(file = "chao_model_simulation/m2/m2_ci_ibp.rds")
list_kmn_pred_test_sp <- readRDS(file = "chao_model_simulation/m2/m2_ci_sp.rds")

###### 3.b) Read results: CI for insample (Poiss/NB/Gamma) ################
list_kn_rarefaction_poiss <- readRDS(file = "chao_model_simulation/m2/m2_ci_insample_poiss.rds")
list_kn_rarefaction_negbin <- readRDS(file = "chao_model_simulation/m2/m2_ci_insample_negbin.rds")
list_kn_rarefaction_ibp <- readRDS(file = "chao_model_simulation/m2/m2_ci_insample_ibp.rds")
list_kn_rarefaction_sp <- readRDS(file = "chao_model_simulation/m2/m2_ci_insample_sp.rds")

###### 4) Read the data ###############################
data_mat <- readRDS(file = "chao_model_simulation/m2/m2_data_mat.rds")
data_list <- create_features_list(data_mat)
L <- nrow(data_mat)
Ms <- sapply(list_kmn_pred_test_poiss, function(l) length(l$medians))
Ns <- L - Ms


##### Accuracy on multiple datasets #####

###### 1) Read results: limit distribution estimates #####
load(file = "chao_model_simulation/m2/m2_avg_ntilde_poiss.Rda")
load(file = "chao_model_simulation/m2/m2_avg_ntilde_negbin.Rda")

###### 2) Read results: quantities on accuracy #####
load(file = "chao_model_simulation/m2/m2_obs_train.Rda")
load(file = "chao_model_simulation/m2/m2_obs_new.Rda")

load(file = "chao_model_simulation/m2/m2_est_new_poiss.Rda")
load(file = "chao_model_simulation/m2/m2_est_new_negbin.Rda")
load(file = "chao_model_simulation/m2/m2_est_new_ibp.Rda")
load(file = "chao_model_simulation/m2/m2_est_new_sp.Rda")

D <- nrow(avg_ntilde_poiss)

```

Set $\pi_k = c\cdot a_k$, for $k =1,\ldots,H$, and $a_k \overset{iid}{\sim} {\rm Uniform}(0,1)$. Set $c$ such that the maximum $\pi_k$ is equal to $0.5$. Let $L$ be the total dimension of the dataset, and consider different dimensions for the training set $n$, i.e. 

```{r, echo=FALSE}
# Total dimension of the dataset
cat("L = ",L, "\n", sep = "\t")
# Different dimensions of the training set
cat("n = ",as.numeric(Ns), "\n", sep = "\t")
```

Here, the curve representing the number of observed features in increasing samples, where the grey vertical lines indicate the different training dimensions.

```{r, echo=FALSE, fig.align = "center"}

plot_trajectory(data_list) + 
  ggtitle("Number of observed features in increasing samples") +
  geom_vline(xintercept = Ns, color="grey", linetype="dashed", linewidth=1)

```
\newpage
Here, we report (i.a) the in-sample rarefaction curve (on a single dataset).

```{r, echo=FALSE}
gg_kn_rarefaction_all  <- vector(mode="list", length = length(Ns))

for (j in 1:length(Ns)){
  N <- Ns[j]
  
  train_mat <- data_mat[1:N,]
  # convert the binary matrix into list of features
  train_list <- create_features_list(train_mat)
  
  gg_kn_rarefaction_all[[j]] <- plot_Kn_median_and_rarefaction_all(
    train_list = train_list,
    ci_poiss = list_kn_rarefaction_poiss[[paste0("N.",N)]], 
    ci_negbin = list_kn_rarefaction_negbin[[paste0("N.",N)]],
    ci_ibp = list_kn_rarefaction_ibp[[paste0("N.",N)]],
    n_avg = 100) +
    ggtitle(paste0("n = ", N)) + theme(plot.title = element_text(size=12))
  
  if (j != 1){
    gg_kn_rarefaction_all[[j]] <- gg_kn_rarefaction_all[[j]] +
      theme(#axis.text.y = element_blank(),
            #axis.ticks.y = element_blank(),
            axis.title.y = element_blank() )
  }
  
}
```

```{r, echo=FALSE, fig.align = "center"}
# Print plots
fig_m1_rare_ <- wrap_plots(gg_kn_rarefaction_all, nrow = 1, ncol = 4) +   plot_layout(guides = "collect") & theme(legend.position = 'right') & xlab(NULL) & theme(plot.margin = margin(5.5, 5.5, 0, 5.5))

fig_m1_rare <- wrap_elements(panel = fig_m1_rare_) +
  labs(tag = "# observations") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom") 

ggsave(filename = "Plots_paper/plot_m2_rare.png", width = 10, height = 4, dpi = 300, units = "in", device='png')
```
Here, we report (i.b) the extrapolated rarefaction curve (on a single dataset).

```{r, echo=FALSE}
gg_kmn_pred_test_all  <- vector(mode="list", length = length(Ns))

for (j in 1:length(Ns)){
  N <- Ns[j]
  M <- L - N
  
  train_mat <- data_mat[1:N,]
  test_mat <- data_mat[(N+1):L, ]
  # convert the binary matrix into list of features
  train_list <- create_features_list(train_mat)
  test_list <- create_features_list(test_mat)
  
  
  gg_kmn_pred_test_all[[j]] <- plot_Kmn_median_pred_and_test_all(
    train_list = train_list,
    test_list = test_list,
    ci_poiss = list_kmn_pred_test_poiss[[paste0("N.",N)]], 
    ci_negbin = list_kmn_pred_test_negbin[[paste0("N.",N)]],
    ci_ibp = list_kmn_pred_test_ibp[[paste0("N.",N)]],
    #ci_sp = list_kmn_pred_test_sp[[paste0("N.",N)]],
    n_avg = 100) +
    ggtitle(paste0("n = ", N)) + theme(plot.title = element_text(size=12))
  
  if (j != 1){
    gg_kmn_pred_test_all[[j]] <- gg_kmn_pred_test_all[[j]] +
      theme(#axis.text.y = element_blank(),
            #axis.ticks.y = element_blank(),
            axis.title.y = element_blank() )
  }
  
}

```


```{r, echo=FALSE, fig.align = "center"}
# Print plots
fig_m2_pred_ <- wrap_plots(gg_kmn_pred_test_all, nrow = 1, ncol = 4) +   plot_layout(guides = "collect") & theme(legend.position = 'right') & xlab(NULL) & theme(plot.margin = margin(5.5, 5.5, 0, 5.5))

fig_m2_pred <- wrap_elements(panel = fig_m2_pred_) +
  labs(tag = "# observations") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom") 

ggsave(filename = "Plots_paper/plot_m2_pred.png", width = 10, height = 4, dpi = 300, units = "in", device='png')

```

Here, we report (ii) the accuracy of the estimated number of unseen features in the test sample, over $D=50$ replicated datasets. 

```{r, echo=FALSE, fig.align = "center"}
# Print plots
acc_alt_poiss <- compute_accuracy(obs_new, est_new_poiss, obs_train)

acc_alt_negbin <- compute_accuracy(obs_new, est_new_negbin, obs_train)

acc_alt_ibp <- compute_accuracy(obs_new, est_new_ibp, obs_train)

#acc_alt_sp <- compute_accuracy(obs_new, est_new_sp, obs_train)


acc_alt_poiss_long <- gather(acc_alt_poiss, training, Accuracy, 
                             paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "BBmixP", N = rep(Ns, each = D))

acc_alt_negbin_long <- gather(acc_alt_negbin, training , Accuracy, 
                              paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "BBmixNB", N = rep(Ns, each = D))

acc_alt_ibp_long <- gather(acc_alt_ibp, training , Accuracy, 
                           paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "3IBPmix", N = rep(Ns, each = D))

#acc_alt_sp_long <- gather(acc_alt_sp, training , Accuracy,                           paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%  add_column(Model = "SB-SP", N = rep(Ns, each = D))

joint_alt_long <- bind_rows(acc_alt_poiss_long, acc_alt_negbin_long, 
                            acc_alt_ibp_long) %>%
    mutate(Model = fct_relevel(Model, c("3IBPmix", "BBmixP", "BBmixNB"))) 


# plots
ggplot(joint_alt_long, aes( y=Accuracy, x=Model, fill=Model)) + 
  geom_boxplot() + 
  facet_wrap(~N, labeller = labeller(N = ~ paste("n = ", .x)), scales = "free", nrow = 1) +
  theme_bw() +
  theme(strip.text.x = element_text(size = 12))+
  theme(#panel.grid.major = element_blank(),
        #panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = pretty_breaks()) +
  rremove("xlab") +
  scale_fill_manual(values = c("3IBPmix" = "orangered1" ,
                                  "BBmixP" = "forestgreen",
                                  "BBmixNB" = "royalblue1")) 

ggsave(filename = "Plots_paper/plot_m2_accuracy.png", width = 10, height = 4, dpi = 300, units = "in", device='png')

```
Even if the Mixture of IBP seems to reach better performance when the training set increases, this is just due to the fact the test set dimension is reducing: see the behavior of the extrapolated rarefaction curve to get the behavior of the model on larger test sets.

\newpage
For the mixtures of Beta-Bernoulli, we report (iii) the posterior distribution of the total number of features (on a single dataset).


```{r, echo=FALSE, fig.align = "center"}


gg_ntilde_poiss_long <- gather(gg_ntilde_poiss, training, estimate, 
                               paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), 
                               factor_key=TRUE) %>%
  add_column(Model= "BBmixP", N = rep(Ns, each = nrow(gg_ntilde_poiss))) %>% 
  filter(estimate < 2000)


gg_ntilde_negbin_long <- gather(gg_ntilde_negbin, training, estimate, 
                                paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), 
                                factor_key=TRUE) %>%
  add_column(Model= "BBmixNB", N = rep(Ns, each = nrow(gg_ntilde_negbin))) %>% 
  filter(estimate < 2000)


joint_total_long <- bind_rows(gg_ntilde_poiss_long, gg_ntilde_negbin_long) %>%
    mutate(Model = fct_relevel(Model, c("BBmixP", "BBmixNB"))) 

# plot
ggplot(joint_total_long, aes(x = estimate, color = Model)) + 
  #geom_density(alpha = 0.5, linewidth = 0.8) +
  stat_density( aes(x=estimate, colour=Model), bw = 0.8,
                     geom="line",position="identity") +
  geom_vline(xintercept = 500, color="black", linetype="dashed", linewidth=0.8) +
  facet_wrap(~N, labeller = labeller(N = ~ paste("n = ", .x)), scales = "free", nrow = 1) +
  theme_bw() + 
  theme(strip.text.x = element_text(size = 12))+
  theme(#panel.grid.major = element_blank(),
        #panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_y_continuous(breaks = pretty_breaks()) +
  xlab("# distinct features") + rremove("ylab") +
  scale_color_manual(values = c("BBmixP" = "forestgreen",
                                  "BBmixNB" = "royalblue1")) 

ggsave(filename = "Plots_paper/plot_m2_richness.png", width = 10, height = 4, dpi = 300, units = "in", device='png')


```

Finally, we report (iv) the expected value of the posterior distribution of the total number of features, over $D=50$ replicated datasets.

```{r, echo=FALSE, fig.align = "center"}
# Plot limit distribution estimate
avg_ntilde_poiss_long <- gather(avg_ntilde_poiss, training, estimate, 
                                paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model = "BBmixP", N = rep(Ns[1:length(Ns)], each = D))

avg_ntilde_negbin_long <- gather(avg_ntilde_negbin, training, estimate, 
                                 paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model = "BBmixNB", N = rep(Ns[1:length(Ns)], each = D))

joint_ntilde_long <- bind_rows(avg_ntilde_poiss_long, avg_ntilde_negbin_long) %>%
      mutate(Model = fct_relevel(Model, c("BBmixP", "BBmixNB"))) 


# plots
ggplot(joint_ntilde_long, aes( y=estimate, x=Model, fill=Model)) + 
  geom_boxplot() + 
  geom_hline(yintercept = 500,color="black", linetype="dashed", linewidth=0.8) +
  facet_wrap(~N, labeller = labeller(N = ~ paste("n = ", .x)), scales = "free", nrow = 1) +
  theme_bw() + 
  theme(strip.text.x = element_text(size = 12))+
  theme(#panel.grid.major = element_blank(),
        #panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = pretty_breaks()) +
  rremove("xlab") + ylab("# distinct features") +
  scale_fill_manual(values = c("BBmixP" = "forestgreen",
                                  "BBmixNB" = "royalblue1")) 

ggsave(filename = "Plots_paper/plot_m2_rep_rich.png", width = 10, height = 4, dpi = 300, units = "in", device='png')

```


#######################################

\newpage
### Scenario 3: the broken stick model

```{r, include=FALSE, eval=TRUE}
####
##### MODEL 3: the broken stick model ############################
####

rm(list=ls())
library(ggmcmc)
library(coda)
library(ggpubr)
library(scales)
library(tidyverse)
library(gridExtra)
library(grid)

##### Single dataset -> Ntilde (Poiss/NB) and extrapolation (Poiss/NB/Gamma) #####

###### 1) Read results:  MCMC convergence ####################
load(file = "chao_model_simulation/m3/m3_params_poiss.Rda")
load(file =  "chao_model_simulation/m3/m3_params_negbin.Rda")
load(file =  "chao_model_simulation/m3/m3_params_ibp.Rda" )
load(file =  "chao_model_simulation/m3/m3_params_sp.Rda" )

###### 2) Read results: samples from limiting distributions (Poiss/NB) ##############
load(file = "chao_model_simulation/m3/m3_ntilde_poiss.Rda")
load(file = "chao_model_simulation/m3/m3_ntilde_negbin.Rda")

###### 3) Read results: CI for extrapolation (Poiss/NB/Gamma) ################
list_kmn_pred_test_poiss <- readRDS(file = "chao_model_simulation/m3/m3_ci_poiss.rds")
list_kmn_pred_test_negbin <- readRDS(file = "chao_model_simulation/m3/m3_ci_negbin.rds")
list_kmn_pred_test_ibp <- readRDS(file = "chao_model_simulation/m3/m3_ci_ibp.rds")
list_kmn_pred_test_sp <- readRDS(file = "chao_model_simulation/m3/m3_ci_sp.rds")

###### 3.b) Read results: CI for insample (Poiss/NB/Gamma) ################
list_kn_rarefaction_poiss <- readRDS(file = "chao_model_simulation/m3/m3_ci_insample_poiss.rds")
list_kn_rarefaction_negbin <- readRDS(file = "chao_model_simulation/m3/m3_ci_insample_negbin.rds")
list_kn_rarefaction_ibp <- readRDS(file = "chao_model_simulation/m3/m3_ci_insample_ibp.rds")
list_kn_rarefaction_sp <- readRDS(file = "chao_model_simulation/m3/m3_ci_insample_sp.rds")

###### 4) Read the data ###############################
data_mat <- readRDS(file = "chao_model_simulation/m3/m3_data_mat.rds")
data_list <- create_features_list(data_mat)
L <- nrow(data_mat)
Ms <- sapply(list_kmn_pred_test_poiss, function(l) length(l$medians))
Ns <- L - Ms


##### Accuracy on multiple datasets #####

###### 1) Read results: limit distribution estimates #####
load(file = "chao_model_simulation/m3/m3_avg_ntilde_poiss.Rda")
load(file = "chao_model_simulation/m3/m3_avg_ntilde_negbin.Rda")

###### 2) Read results: quantities on accuracy #####
load(file = "chao_model_simulation/m3/m3_obs_train.Rda")
load(file = "chao_model_simulation/m3/m3_obs_new.Rda")

load(file = "chao_model_simulation/m3/m3_est_new_poiss.Rda")
load(file = "chao_model_simulation/m3/m3_est_new_negbin.Rda")
load(file = "chao_model_simulation/m3/m3_est_new_ibp.Rda")
load(file = "chao_model_simulation/m3/m3_est_new_sp.Rda")

D <- nrow(avg_ntilde_poiss)

```

Set $\pi_k = c\cdot a_k$, for $k =1,\ldots,H$, and $a_k \overset{iid}{\sim} {\rm Exp}(1)$. Set $c$ such that the maximum $\pi_k$ is equal to $0.5$. As Chiu (2022) says: "This model is commonly used in previous literature
and equivalent to the Dirichlet distribution". Let $L$ be the total dimension of the dataset, and consider different dimensions for the training set $n$, i.e. 

```{r, echo=FALSE}
# Total dimension of the dataset
cat("L = ",L, "\n", sep = "\t")
# Different dimensions of the training set
cat("n = ",as.numeric(Ns), "\n", sep = "\t")
```

Here, the curve representing the number of observed features in increasing samples, where the grey vertical lines indicate the different training dimensions.

```{r, echo=FALSE, fig.align = "center"}

plot_trajectory(data_list) + 
  ggtitle("Number of observed features in increasing samples") +
  geom_vline(xintercept = Ns, color="grey", linetype="dashed", linewidth=1)

```
\newpage
Here, we report (i.a) the in-sample rarefaction curve (on a single dataset).

```{r, echo=FALSE}
gg_kn_rarefaction_all  <- vector(mode="list", length = length(Ns))

for (j in 1:length(Ns)){
  N <- Ns[j]
  
  train_mat <- data_mat[1:N,]
  # convert the binary matrix into list of features
  train_list <- create_features_list(train_mat)
  
  gg_kn_rarefaction_all[[j]] <- plot_Kn_median_and_rarefaction_all(
    train_list = train_list,
    ci_poiss = list_kn_rarefaction_poiss[[paste0("N.",N)]], 
    ci_negbin = list_kn_rarefaction_negbin[[paste0("N.",N)]],
    ci_ibp = list_kn_rarefaction_ibp[[paste0("N.",N)]],
    n_avg = 100) +
    ggtitle(paste0("n = ", N)) + theme(plot.title = element_text(size=12))
  
  if (j != 1){
    gg_kn_rarefaction_all[[j]] <- gg_kn_rarefaction_all[[j]] +
      theme(#axis.text.y = element_blank(),
            #axis.ticks.y = element_blank(),
            axis.title.y = element_blank() )
  }
  
}
```

```{r, echo=FALSE, fig.align = "center"}
# Print plots
fig_m1_rare_ <- wrap_plots(gg_kn_rarefaction_all, nrow = 1, ncol = 4) +   plot_layout(guides = "collect") & theme(legend.position = 'right') & xlab(NULL) & theme(plot.margin = margin(5.5, 5.5, 0, 5.5))

fig_m1_rare <- wrap_elements(panel = fig_m1_rare_) +
  labs(tag = "# observations") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom") 

ggsave(filename = "Plots_paper/plot_m3_rare.png", width = 10, height = 4, dpi = 300, units = "in", device='png')
```
Here, we report (i.b) the extrapolated rarefaction curve (on a single dataset).

```{r, echo=FALSE}
gg_kmn_pred_test_all  <- vector(mode="list", length = length(Ns))

for (j in 1:length(Ns)){
  N <- Ns[j]
  M <- L - N
  
  train_mat <- data_mat[1:N,]
  test_mat <- data_mat[(N+1):L, ]
  # convert the binary matrix into list of features
  train_list <- create_features_list(train_mat)
  test_list <- create_features_list(test_mat)
  
  
  gg_kmn_pred_test_all[[j]] <- plot_Kmn_median_pred_and_test_all(
    train_list = train_list,
    test_list = test_list,
    ci_poiss = list_kmn_pred_test_poiss[[paste0("N.",N)]], 
    ci_negbin = list_kmn_pred_test_negbin[[paste0("N.",N)]],
    ci_ibp = list_kmn_pred_test_ibp[[paste0("N.",N)]],
    #ci_sp = list_kmn_pred_test_sp[[paste0("N.",N)]],
    n_avg = 100) +
    ggtitle(paste0("n = ", N)) + theme(plot.title = element_text(size=12))
  
  if (j != 1){
    gg_kmn_pred_test_all[[j]] <- gg_kmn_pred_test_all[[j]] +
      theme(#axis.text.y = element_blank(),
            #axis.ticks.y = element_blank(),
            axis.title.y = element_blank() )
  }
  
}

```


```{r, echo=FALSE, fig.align = "center"}
# Print plots
fig_m3_pred_ <- wrap_plots(gg_kmn_pred_test_all, nrow = 1, ncol = 4) +   plot_layout(guides = "collect") & theme(legend.position = 'right') & xlab(NULL) & theme(plot.margin = margin(5.5, 5.5, 0, 5.5))

fig_m3_pred <- wrap_elements(panel = fig_m3_pred_) +
  labs(tag = "# observations") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom") 

ggsave(filename = "Plots_paper/plot_m3_pred.png", width = 10, height = 4, dpi = 300, units = "in", device='png')

```

Here, we report (ii) the accuracy of the estimated number of unseen features in the test sample, over $D=50$ replicated datasets. 

```{r, echo=FALSE, fig.align = "center"}
# Print plots
acc_alt_poiss <- compute_accuracy(obs_new, est_new_poiss, obs_train)

acc_alt_negbin <- compute_accuracy(obs_new, est_new_negbin, obs_train)

acc_alt_ibp <- compute_accuracy(obs_new, est_new_ibp, obs_train)

#acc_alt_sp <- compute_accuracy(obs_new, est_new_sp, obs_train)


acc_alt_poiss_long <- gather(acc_alt_poiss, training, Accuracy, 
                             paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "BBmixP", N = rep(Ns, each = D))

acc_alt_negbin_long <- gather(acc_alt_negbin, training , Accuracy, 
                              paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "BBmixNB", N = rep(Ns, each = D))

acc_alt_ibp_long <- gather(acc_alt_ibp, training , Accuracy, 
                           paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "3IBPmix", N = rep(Ns, each = D))

#acc_alt_sp_long <- gather(acc_alt_sp, training , Accuracy,                           paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%  add_column(Model = "SB-SP", N = rep(Ns, each = D))

joint_alt_long <- bind_rows(acc_alt_poiss_long, acc_alt_negbin_long, 
                            acc_alt_ibp_long) %>%
    mutate(Model = fct_relevel(Model, c("3IBPmix", "BBmixP", "BBmixNB"))) 


# plots
ggplot(joint_alt_long, aes( y=Accuracy, x=Model, fill=Model)) + 
  geom_boxplot() + 
  facet_wrap(~N, labeller = labeller(N = ~ paste("n = ", .x)), scales = "free", nrow = 1) +
  theme_bw() + 
  theme(strip.text.x = element_text(size = 12))+
  theme(#panel.grid.major = element_blank(),
        #panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = pretty_breaks()) +
  rremove("xlab") +
  scale_fill_manual(values = c("3IBPmix" = "orangered1" ,
                                  "BBmixP" = "forestgreen",
                                  "BBmixNB" = "royalblue1")) 

ggsave(filename = "Plots_paper/plot_m3_accuracy.png", width = 10, height = 4, dpi = 300, units = "in", device='png')


```
Even if the Mixture of IBP seems to reach better performance when the training set increases, this is just due to the fact the test set dimension is reducing: see the behavior of the extrapolated rarefaction curve to get the behavior of the model on larger test sets.

\newpage
For the mixtures of Beta-Bernoulli, we report (iii) the posterior distribution of the total number of features (on a single dataset).


```{r, echo=FALSE, fig.align = "center"}


gg_ntilde_poiss_long <- gather(gg_ntilde_poiss, training, estimate, 
                               paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), 
                               factor_key=TRUE) %>%
  add_column(Model= "BBmixP", N = rep(Ns, each = nrow(gg_ntilde_poiss))) %>% 
  filter(estimate < 1200)


gg_ntilde_negbin_long <- gather(gg_ntilde_negbin, training, estimate, 
                                paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), 
                                factor_key=TRUE) %>%
  add_column(Model= "BBmixNB", N = rep(Ns, each = nrow(gg_ntilde_negbin))) %>% 
  filter(estimate < 1200)


joint_total_long <- bind_rows(gg_ntilde_poiss_long, gg_ntilde_negbin_long) %>%
    mutate(Model = fct_relevel(Model, c("BBmixP", "BBmixNB"))) 

# plot
ggplot(joint_total_long, aes(x = estimate, color = Model)) + 
  #geom_density(alpha = 0.5, linewidth = 0.8) +
  stat_density(aes(x=estimate, colour=Model),
                     geom="line",position="identity") +
  geom_vline(xintercept = 500, color="black", linetype="dashed", linewidth=0.8) +
  facet_wrap(~N, labeller = labeller(N = ~ paste("n = ", .x)), scales = "free", nrow = 1) +
  theme_bw() + 
  theme(strip.text.x = element_text(size = 12))+
  theme(#panel.grid.major = element_blank(),
        #panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_y_continuous(breaks = pretty_breaks()) +
  xlab("# distinct features") + rremove("ylab") +
  scale_color_manual(values = c("BBmixP" = "forestgreen",
                                  "BBmixNB" = "royalblue1")) 

ggsave(filename = "Plots_paper/plot_m3_richness.png", width = 10, height = 4, dpi = 300, units = "in", device='png')


```

Finally, we report (iv) the expected value of the posterior distribution of the total number of features, over $D=50$ replicated datasets.

```{r, echo=FALSE, fig.align = "center"}
# Plot limit distribution estimate
avg_ntilde_poiss_long <- gather(avg_ntilde_poiss, training, estimate, 
                                paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model = "BBmixP", N = rep(Ns[1:length(Ns)], each = D))

avg_ntilde_negbin_long <- gather(avg_ntilde_negbin, training, estimate, 
                                 paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model = "BBmixNB", N = rep(Ns[1:length(Ns)], each = D))

joint_ntilde_long <- bind_rows(avg_ntilde_poiss_long, avg_ntilde_negbin_long) %>%
      mutate(Model = fct_relevel(Model, c("BBmixP", "BBmixNB"))) 


# plots
ggplot(joint_ntilde_long, aes( y=estimate, x=Model, fill=Model)) + 
  geom_boxplot() + 
  geom_hline(yintercept = 500,color="black", linetype="dashed", linewidth=0.8) +
  facet_wrap(~N, labeller = labeller(N = ~ paste("n = ", .x)), scales = "free", nrow = 1) +
  theme_bw() + 
  theme(strip.text.x = element_text(size = 12))+
  theme(#panel.grid.major = element_blank(),
        #panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = pretty_breaks()) +
  rremove("xlab") + ylab("# distinct features") +
  scale_fill_manual(values = c("BBmixP" = "forestgreen",
                                  "BBmixNB" = "royalblue1")) 

ggsave(filename = "Plots_paper/plot_m3_rep_rich.png", width = 10, height = 4, dpi = 300, units = "in", device='png')


```



#######################################

\newpage
### Scenario 4: the log-normal model

```{r, include=FALSE, eval=TRUE}
####
##### MODEL 4: the log-normal model ############################
####

rm(list=ls())
library(ggmcmc)
library(coda)
library(ggpubr)
library(scales)
library(tidyverse)
library(gridExtra)
library(grid)

##### Single dataset -> Ntilde (Poiss/NB) and extrapolation (Poiss/NB/Gamma) #####

###### 1) Read results:  MCMC convergence ####################
load(file = "chao_model_simulation/m4/m4_params_poiss.Rda")
load(file =  "chao_model_simulation/m4/m4_params_negbin.Rda")
load(file =  "chao_model_simulation/m4/m4_params_ibp.Rda" )
load(file =  "chao_model_simulation/m4/m4_params_sp.Rda" )

###### 2) Read results: samples from limiting distributions (Poiss/NB) ##############
load(file = "chao_model_simulation/m4/m4_ntilde_poiss.Rda")
load(file = "chao_model_simulation/m4/m4_ntilde_negbin.Rda")

###### 3) Read results: CI for extrapolation (Poiss/NB/Gamma) ################
list_kmn_pred_test_poiss <- readRDS(file = "chao_model_simulation/m4/m4_ci_poiss.rds")
list_kmn_pred_test_negbin <- readRDS(file = "chao_model_simulation/m4/m4_ci_negbin.rds")
list_kmn_pred_test_ibp <- readRDS(file = "chao_model_simulation/m4/m4_ci_ibp.rds")
list_kmn_pred_test_sp <- readRDS(file = "chao_model_simulation/m4/m4_ci_sp.rds")

###### 3.b) Read results: CI for insample (Poiss/NB/Gamma) ################
list_kn_rarefaction_poiss <- readRDS(file = "chao_model_simulation/m4/m4_ci_insample_poiss.rds")
list_kn_rarefaction_negbin <- readRDS(file = "chao_model_simulation/m4/m4_ci_insample_negbin.rds")
list_kn_rarefaction_ibp <- readRDS(file = "chao_model_simulation/m4/m4_ci_insample_ibp.rds")
list_kn_rarefaction_sp <- readRDS(file = "chao_model_simulation/m4/m4_ci_insample_sp.rds")

###### 4) Read the data ###############################
data_mat <- readRDS(file = "chao_model_simulation/m4/m4_data_mat.rds")
data_list <- create_features_list(data_mat)
L <- nrow(data_mat)
Ms <- sapply(list_kmn_pred_test_poiss, function(l) length(l$medians))
Ns <- L - Ms



##### Accuracy on multiple datasets #####

###### 1) Read results: limit distribution estimates #####
load(file = "chao_model_simulation/m4/m4_avg_ntilde_poiss.Rda")
load(file = "chao_model_simulation/m4/m4_avg_ntilde_negbin.Rda")

###### 2) Read results: quantities on accuracy #####
load(file = "chao_model_simulation/m4/m4_obs_train.Rda")
load(file = "chao_model_simulation/m4/m4_obs_new.Rda")

load(file = "chao_model_simulation/m4/m4_est_new_poiss.Rda")
load(file = "chao_model_simulation/m4/m4_est_new_negbin.Rda")
load(file = "chao_model_simulation/m4/m4_est_new_ibp.Rda")
load(file = "chao_model_simulation/m4/m4_est_new_sp.Rda")

D <- nrow(avg_ntilde_poiss)

```

Set $\pi_k = c\cdot a_k$, for $k =1,\ldots,H$, and $a_k \overset{iid}{\sim} {\rm lognormal}(0,1)$. Set $c$ such that the maximum $\pi_k$ is equal to $1$. Let $L$ be the total dimension of the dataset, and consider different dimensions for the training set $n$, i.e. 

```{r, echo=FALSE}
# Total dimension of the dataset
cat("L = ",L, "\n", sep = "\t")
# Different dimensions of the training set
cat("n = ",as.numeric(Ns), "\n", sep = "\t")
```

Here, the curve representing the number of observed features in increasing samples, where the grey vertical lines indicate the different training dimensions.

```{r, echo=FALSE, fig.align = "center"}

plot_trajectory(data_list) + 
  ggtitle("Number of observed features in increasing samples") +
  geom_vline(xintercept = Ns, color="grey", linetype="dashed", linewidth=1)

```
\newpage
Here, we report (i.a) the in-sample rarefaction curve (on a single dataset).

```{r, echo=FALSE}
gg_kn_rarefaction_all  <- vector(mode="list", length = length(Ns))

for (j in 1:length(Ns)){
  N <- Ns[j]
  
  train_mat <- data_mat[1:N,]
  # convert the binary matrix into list of features
  train_list <- create_features_list(train_mat)
  
  gg_kn_rarefaction_all[[j]] <- plot_Kn_median_and_rarefaction_all(
    train_list = train_list,
    ci_poiss = list_kn_rarefaction_poiss[[paste0("N.",N)]], 
    ci_negbin = list_kn_rarefaction_negbin[[paste0("N.",N)]],
    ci_ibp = list_kn_rarefaction_ibp[[paste0("N.",N)]],
    n_avg = 100) +
    ggtitle(paste0("n = ", N)) + theme(plot.title = element_text(size=12))
  
  if (j != 1){
    gg_kn_rarefaction_all[[j]] <- gg_kn_rarefaction_all[[j]] +
      theme(#axis.text.y = element_blank(),
            #axis.ticks.y = element_blank(),
            axis.title.y = element_blank() )
  }
  
}
```

```{r, echo=FALSE, fig.align = "center"}
# Print plots
fig_m1_rare_ <- wrap_plots(gg_kn_rarefaction_all, nrow = 1, ncol = 4) +   plot_layout(guides = "collect") & theme(legend.position = 'right') & xlab(NULL) & theme(plot.margin = margin(5.5, 5.5, 0, 5.5))

fig_m1_rare <- wrap_elements(panel = fig_m1_rare_) +
  labs(tag = "# observations") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom") 

ggsave(filename = "Plots_paper/plot_m4_rare.png", width = 10, height = 4, dpi = 300, units = "in", device='png')
```
Here, we report (i.b) the extrapolated rarefaction curve (on a single dataset).

```{r, echo=FALSE}
gg_kmn_pred_test_all  <- vector(mode="list", length = length(Ns))

for (j in 1:length(Ns)){
  N <- Ns[j]
  M <- L - N
  
  train_mat <- data_mat[1:N,]
  test_mat <- data_mat[(N+1):L, ]
  # convert the binary matrix into list of features
  train_list <- create_features_list(train_mat)
  test_list <- create_features_list(test_mat)
  
  
  gg_kmn_pred_test_all[[j]] <- plot_Kmn_median_pred_and_test_all(
    train_list = train_list,
    test_list = test_list,
    ci_poiss = list_kmn_pred_test_poiss[[paste0("N.",N)]], 
    ci_negbin = list_kmn_pred_test_negbin[[paste0("N.",N)]],
    ci_ibp = list_kmn_pred_test_ibp[[paste0("N.",N)]],
    n_avg = 100) +
    ggtitle(paste0("n = ", N)) + theme(plot.title = element_text(size=12))
  
  if (j != 1){
    gg_kmn_pred_test_all[[j]] <- gg_kmn_pred_test_all[[j]] +
      theme(#axis.text.y = element_blank(),
            #axis.ticks.y = element_blank(),
            axis.title.y = element_blank() )
  }
  
}

```


```{r, echo=FALSE, fig.align = "center"}
# Print plots
fig_m4_pred_ <- wrap_plots(gg_kmn_pred_test_all, nrow = 1, ncol = 4) +   plot_layout(guides = "collect") & theme(legend.position = 'right') & xlab(NULL) & theme(plot.margin = margin(5.5, 5.5, 0, 5.5))

fig_m4_pred <- wrap_elements(panel = fig_m4_pred_) +
  labs(tag = "# observations") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom") 

ggsave(filename = "Plots_paper/plot_m4_pred.png", width = 10, height = 4, dpi = 300, units = "in", device='png')

```

Here, we report (ii) the accuracy of the estimated number of unseen features in the test sample, over $D=50$ replicated datasets. 

```{r, echo=FALSE, fig.align = "center"}
# Print plots
acc_alt_poiss <- compute_accuracy(obs_new, est_new_poiss, obs_train)

acc_alt_negbin <- compute_accuracy(obs_new, est_new_negbin, obs_train)

acc_alt_ibp <- compute_accuracy(obs_new, est_new_ibp, obs_train)

#acc_alt_sp <- compute_accuracy(obs_new, est_new_sp, obs_train)


acc_alt_poiss_long <- gather(acc_alt_poiss, training, Accuracy, 
                             paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "BBmixP", N = rep(Ns, each = D))

acc_alt_negbin_long <- gather(acc_alt_negbin, training , Accuracy, 
                              paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "BBmixNB", N = rep(Ns, each = D))

acc_alt_ibp_long <- gather(acc_alt_ibp, training , Accuracy, 
                           paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "3IBPmix", N = rep(Ns, each = D))

#acc_alt_sp_long <- gather(acc_alt_sp, training , Accuracy,                           paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%  add_column(Model = "SB-SP", N = rep(Ns, each = D))

joint_alt_long <- bind_rows(acc_alt_poiss_long, acc_alt_negbin_long, 
                            acc_alt_ibp_long) %>%
    mutate(Model = fct_relevel(Model, c("3IBPmix", "BBmixP", "BBmixNB"))) 


# plots
ggplot(joint_alt_long, aes( y=Accuracy, x=Model, fill=Model)) + 
  geom_boxplot() + 
  facet_wrap(~N, labeller = labeller(N = ~ paste("n = ", .x)), scales = "free", nrow = 1) +
  theme_bw() + 
  theme(strip.text.x = element_text(size = 12))+
  theme(#panel.grid.major = element_blank(),
        #panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = pretty_breaks()) +
  rremove("xlab") +
  scale_fill_manual(values = c("3IBPmix" = "orangered1" ,
                                  "BBmixP" = "forestgreen",
                                  "BBmixNB" = "royalblue1")) 

ggsave(filename = "Plots_paper/plot_m4_accuracy.png", width = 10, height = 4, dpi = 300, units = "in", device='png')

```
Even if the Mixture of IBP seems to reach better performance when the training set increases, this is just due to the fact the test set dimension is reducing: see the behavior of the extrapolated rarefaction curve to get the behavior of the model on larger test sets.

\newpage
For the mixtures of Beta-Bernoulli, we report (iii) the posterior distribution of the total number of features (on a single dataset).


```{r, echo=FALSE, fig.align = "center"}


gg_ntilde_poiss_long <- gather(gg_ntilde_poiss, training, estimate, 
                               paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), 
                               factor_key=TRUE) %>%
  add_column(Model= "BBmixP", N = rep(Ns, each = nrow(gg_ntilde_poiss))) %>% 
  filter(estimate < 2000)


gg_ntilde_negbin_long <- gather(gg_ntilde_negbin, training, estimate, 
                                paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), 
                                factor_key=TRUE) %>%
  add_column(Model= "BBmixNB", N = rep(Ns, each = nrow(gg_ntilde_negbin))) %>% 
  filter(estimate < 2000)


joint_total_long <- bind_rows(gg_ntilde_poiss_long, gg_ntilde_negbin_long) %>%
    mutate(Model = fct_relevel(Model, c("BBmixP", "BBmixNB"))) 

# plot
ggplot(joint_total_long, aes(x = estimate, color = Model)) + 
  #geom_density(alpha = 0.5, linewidth = 0.8) +
  stat_density(aes(x=estimate, colour=Model),
                     geom="line",position="identity") +
  geom_vline(xintercept = 500, color="black", linetype="dashed", linewidth=0.8) +
  facet_wrap(~N, labeller = labeller(N = ~ paste("n = ", .x)), scales = "free", nrow = 1) +
  theme_bw() + 
  theme(strip.text.x = element_text(size = 12))+
  theme(#panel.grid.major = element_blank(),
        #panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_y_continuous(breaks = pretty_breaks()) +
  xlab("# distinct features") + rremove("ylab") +
  scale_color_manual(values = c("BBmixP" = "forestgreen",
                                  "BBmixNB" = "royalblue1")) 

ggsave(filename = "Plots_paper/plot_m4_richness.png", width = 10, height = 4, dpi = 300, units = "in", device='png')


```

Finally, we report (iv) the expected value of the posterior distribution of the total number of features, over $D=50$ replicated datasets.

```{r, echo=FALSE, fig.align = "center"}
# Plot limit distribution estimate
avg_ntilde_poiss_long <- gather(avg_ntilde_poiss, training, estimate, 
                                paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model = "BBmixP", N = rep(Ns[1:length(Ns)], each = D))

avg_ntilde_negbin_long <- gather(avg_ntilde_negbin, training, estimate, 
                                 paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model = "BBmixNB", N = rep(Ns[1:length(Ns)], each = D))

joint_ntilde_long <- bind_rows(avg_ntilde_poiss_long, avg_ntilde_negbin_long) %>%
      mutate(Model = fct_relevel(Model, c("BBmixP", "BBmixNB"))) 


# plots
ggplot(joint_ntilde_long, aes( y=estimate, x=Model, fill=Model)) + 
  geom_boxplot() + 
  geom_hline(yintercept = 500,color="black", linetype="dashed", linewidth=0.8) +
  facet_wrap(~N, labeller = labeller(N = ~ paste("n = ", .x)), scales = "free", nrow = 1) +
  theme_bw() + 
  theme(strip.text.x = element_text(size = 12))+
  theme(#panel.grid.major = element_blank(),
        #panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = pretty_breaks()) +
  rremove("xlab") + ylab("# distinct features") +
  scale_fill_manual(values = c("BBmixP" = "forestgreen",
                                  "BBmixNB" = "royalblue1")) 

ggsave(filename = "Plots_paper/plot_m4_rep_rich.png", width = 10, height = 4, dpi = 300, units = "in", device='png')


```



#######################################

\newpage
### Scenario 5: the Zipf–Mandelbrot model

```{r, include=FALSE, eval=TRUE}
####
##### MODEL 5: the Zipf–Mandelbrot model ############################
####

rm(list=ls())
library(ggmcmc)
library(coda)
library(ggpubr)
library(scales)
library(tidyverse)
library(gridExtra)
library(grid)

##### Single dataset -> Ntilde (Poiss/NB) and extrapolation (Poiss/NB/Gamma) #####

###### 1) Read results:  MCMC convergence ####################
load(file = "chao_model_simulation/m5/m5_params_poiss.Rda")
load(file =  "chao_model_simulation/m5/m5_params_negbin.Rda")
load(file =  "chao_model_simulation/m5/m5_params_ibp.Rda" )
load(file =  "chao_model_simulation/m5/m5_params_sp.Rda" )

###### 2) Read results: samples from limiting distributions (Poiss/NB) ##############
load(file = "chao_model_simulation/m5/m5_ntilde_poiss.Rda")
load(file = "chao_model_simulation/m5/m5_ntilde_negbin.Rda")

###### 3) Read results: CI for extrapolation (Poiss/NB/Gamma) ################
list_kmn_pred_test_poiss <- readRDS(file = "chao_model_simulation/m5/m5_ci_poiss.rds")
list_kmn_pred_test_negbin <- readRDS(file = "chao_model_simulation/m5/m5_ci_negbin.rds")
list_kmn_pred_test_ibp <- readRDS(file = "chao_model_simulation/m5/m5_ci_ibp.rds")
list_kmn_pred_test_sp <- readRDS(file = "chao_model_simulation/m5/m5_ci_sp.rds")

###### 3.b) Read results: CI for insample (Poiss/NB/Gamma) ################
list_kn_rarefaction_poiss <- readRDS(file = "chao_model_simulation/m5/m5_ci_insample_poiss.rds")
list_kn_rarefaction_negbin <- readRDS(file = "chao_model_simulation/m5/m5_ci_insample_negbin.rds")
list_kn_rarefaction_ibp <- readRDS(file = "chao_model_simulation/m5/m5_ci_insample_ibp.rds")
list_kn_rarefaction_sp <- readRDS(file = "chao_model_simulation/m5/m5_ci_insample_sp.rds")

###### 4) Read the data ###############################
data_mat <- readRDS(file = "chao_model_simulation/m5/m5_data_mat.rds")
data_list <- create_features_list(data_mat)
L <- nrow(data_mat)
Ms <- sapply(list_kmn_pred_test_poiss, function(l) length(l$medians))
Ns <- L - Ms



##### Accuracy on multiple datasets #####

###### 1) Read results: limit distribution estimates #####
load(file = "chao_model_simulation/m5/m5_avg_ntilde_poiss.Rda")
load(file = "chao_model_simulation/m5/m5_avg_ntilde_negbin.Rda")

###### 2) Read results: quantities on accuracy #####
load(file = "chao_model_simulation/m5/m5_obs_train.Rda")
load(file = "chao_model_simulation/m5/m5_obs_new.Rda")

load(file = "chao_model_simulation/m5/m5_est_new_poiss.Rda")
load(file = "chao_model_simulation/m5/m5_est_new_negbin.Rda")
load(file = "chao_model_simulation/m5/m5_est_new_ibp.Rda")
load(file = "chao_model_simulation/m5/m5_est_new_sp.Rda")

D <- nrow(avg_ntilde_poiss)

```

Set $\pi_k = \frac{3}{k + 5}$, for $k =1,\ldots,H$. Note that the maximum $\pi_k$ is equal to $0.5$. Let $L$ be the total dimension of the dataset, and consider different dimensions for the training set $n$, i.e. 

```{r, echo=FALSE}
# Total dimension of the dataset
cat("L = ",L, "\n", sep = "\t")
# Different dimensions of the training set
cat("n = ",as.numeric(Ns), "\n", sep = "\t")
```

Here, the curve representing the number of observed features in increasing samples, where the grey vertical lines indicate the different training dimensions.

```{r, echo=FALSE, fig.align = "center"}

plot_trajectory(data_list) + 
  ggtitle("Number of observed features in increasing samples") +
  geom_vline(xintercept = Ns, color="grey", linetype="dashed", linewidth=1)

```
\newpage
Here, we report (i.a) the in-sample rarefaction curve (on a single dataset).

```{r, echo=FALSE}
gg_kn_rarefaction_all  <- vector(mode="list", length = length(Ns))

for (j in 1:length(Ns)){
  N <- Ns[j]
  
  train_mat <- data_mat[1:N,]
  # convert the binary matrix into list of features
  train_list <- create_features_list(train_mat)
  
  gg_kn_rarefaction_all[[j]] <- plot_Kn_median_and_rarefaction_all(
    train_list = train_list,
    ci_poiss = list_kn_rarefaction_poiss[[paste0("N.",N)]], 
    ci_negbin = list_kn_rarefaction_negbin[[paste0("N.",N)]],
    ci_ibp = list_kn_rarefaction_ibp[[paste0("N.",N)]],
    n_avg = 100) +
    ggtitle(paste0("n = ", N)) + theme(plot.title = element_text(size=12))
  
  if (j != 1){
    gg_kn_rarefaction_all[[j]] <- gg_kn_rarefaction_all[[j]] +
      theme(#axis.text.y = element_blank(),
            #axis.ticks.y = element_blank(),
            axis.title.y = element_blank() )
  }
  
}
```

```{r, echo=FALSE, fig.align = "center"}
# Print plots
fig_m1_rare_ <- wrap_plots(gg_kn_rarefaction_all, nrow = 1, ncol = 5) +   plot_layout(guides = "collect") & theme(legend.position = 'right') & xlab(NULL) & theme(plot.margin = margin(5.5, 5.5, 0, 5.5))

fig_m1_rare <- wrap_elements(panel = fig_m1_rare_) +
  labs(tag = "# observations") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom") 

ggsave(filename = "Plots_paper/plot_m5_rare.png", width = 10, height = 4, dpi = 300, units = "in", device='png')
```

\newpage
Here, we report (i.b) the extrapolated rarefaction curve (on a single dataset).

```{r, echo=FALSE}
gg_kmn_pred_test_all  <- vector(mode="list", length = length(Ns))

for (j in 1:length(Ns)){
  N <- Ns[j]
  M <- L - N
  
  train_mat <- data_mat[1:N,]
  test_mat <- data_mat[(N+1):L, ]
  # convert the binary matrix into list of features
  train_list <- create_features_list(train_mat)
  test_list <- create_features_list(test_mat)
  
  
  gg_kmn_pred_test_all[[j]] <- plot_Kmn_median_pred_and_test_all(
    train_list = train_list,
    test_list = test_list,
    ci_poiss = list_kmn_pred_test_poiss[[paste0("N.",N)]], 
    ci_negbin = list_kmn_pred_test_negbin[[paste0("N.",N)]],
    ci_ibp = list_kmn_pred_test_ibp[[paste0("N.",N)]],
    n_avg = 100) +
    ggtitle(paste0("n = ", N)) + theme(plot.title = element_text(size=12))
  
if (j != 1){
    gg_kmn_pred_test_all[[j]] <- gg_kmn_pred_test_all[[j]] +
      theme(#axis.text.y = element_blank(),
            #axis.ticks.y = element_blank(),
            axis.title.y = element_blank() )
  }
  
}

```


```{r, echo=FALSE, fig.align = "center"}
# Print plots
fig_m5_pred_ <- wrap_plots(gg_kmn_pred_test_all, nrow = 1, ncol = 5) +   plot_layout(guides = "collect") & theme(legend.position = 'right') & xlab(NULL) & theme(plot.margin = margin(5.5, 5.5, 0, 5.5))

fig_m5_pred <- wrap_elements(panel = fig_m5_pred_) +
  labs(tag = "# observations") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom") 

ggsave(filename = "Plots_paper/plot_m5_pred.png", width = 10, height = 4, dpi = 300, units = "in", device='png')

```

\newpage
Here, we report (ii) the accuracy of the estimated number of unseen features in the test sample, over $D=50$ replicated datasets. 

```{r, echo=FALSE, fig.align = "center"}
# Print plots
acc_alt_poiss <- compute_accuracy(obs_new, est_new_poiss, obs_train)

acc_alt_negbin <- compute_accuracy(obs_new, est_new_negbin, obs_train)

acc_alt_ibp <- compute_accuracy(obs_new, est_new_ibp, obs_train)

#acc_alt_sp <- compute_accuracy(obs_new, est_new_sp, obs_train)


acc_alt_poiss_long <- gather(acc_alt_poiss, training, Accuracy, 
                             paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "BBmixP", N = rep(Ns, each = D))

acc_alt_negbin_long <- gather(acc_alt_negbin, training , Accuracy, 
                              paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "BBmixNB", N = rep(Ns, each = D))

acc_alt_ibp_long <- gather(acc_alt_ibp, training , Accuracy, 
                           paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "3IBPmix", N = rep(Ns, each = D))

#acc_alt_sp_long <- gather(acc_alt_sp, training , Accuracy,                           paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%  add_column(Model = "SB-SP", N = rep(Ns, each = D))

joint_alt_long <- bind_rows(acc_alt_poiss_long, acc_alt_negbin_long, 
                            acc_alt_ibp_long) %>%
    mutate(Model = fct_relevel(Model, c("3IBPmix", "BBmixP", "BBmixNB"))) 


# plots
ggplot(joint_alt_long, aes( y=Accuracy, x=Model, fill=Model)) + 
  geom_boxplot() + 
  facet_wrap(~N, labeller = labeller(N = ~ paste("n = ", .x)), scales = "free", nrow = 1) +
  theme_bw() + 
  theme(strip.text.x = element_text(size = 12))+
  theme(#panel.grid.major = element_blank(),
        #panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = pretty_breaks()) +
  rremove("xlab") +
  scale_fill_manual(values = c("3IBPmix" = "orangered1" ,
                                  "BBmixP" = "forestgreen",
                                  "BBmixNB" = "royalblue1")) 

ggsave(filename = "Plots_paper/plot_m5_accuracy.png", width = 10, height = 4, dpi = 300, units = "in", device='png')


```
\newpage
For the mixtures of Beta-Bernoulli, we report (iii) the posterior distribution of the total number of features (on a single dataset).


```{r, echo=FALSE, fig.align = "center"}


gg_ntilde_poiss_long <- gather(gg_ntilde_poiss, training, estimate, 
                               paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), 
                               factor_key=TRUE) %>%
  add_column(Model= "BBmixP", N = rep(Ns, each = nrow(gg_ntilde_poiss))) %>% 
  filter(estimate < 2500)


gg_ntilde_negbin_long <- gather(gg_ntilde_negbin, training, estimate, 
                                paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), 
                                factor_key=TRUE) %>%
  add_column(Model= "BBmixNB", N = rep(Ns, each = nrow(gg_ntilde_negbin))) %>% 
  filter(estimate < 2500)


joint_total_long <- bind_rows(gg_ntilde_poiss_long, gg_ntilde_negbin_long) %>%
    mutate(Model = fct_relevel(Model, c("BBmixP", "BBmixNB"))) 

# plot
ggplot(joint_total_long, aes(x = estimate, color = Model)) + 
  #geom_density(alpha = 0.5, linewidth = 0.8) +
  stat_density(aes(x=estimate, colour=Model), bw = 50,
                     geom="line",position="identity") +
  geom_vline(xintercept = 500, color="black", linetype="dashed", linewidth=0.8) +
  facet_wrap(~N, labeller = labeller(N = ~ paste("n = ", .x)), scales = "free", nrow = 1) +
  theme_bw() + 
  theme(strip.text.x = element_text(size = 12))+
  theme(#panel.grid.major = element_blank(),
        #panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_y_continuous(breaks = pretty_breaks()) +
  xlab("# distinct features") + rremove("ylab") +
  scale_color_manual(values = c("BBmixP" = "forestgreen",
                                  "BBmixNB" = "royalblue1")) 

ggsave(filename = "Plots_paper/plot_m5_richness.png", width = 10, height = 4, dpi = 300, units = "in", device='png')


```

Finally, we report (iv) the expected value of the posterior distribution of the total number of features, over $D=50$ replicated datasets.

```{r, echo=FALSE, fig.align = "center"}
# Plot limit distribution estimate
avg_ntilde_poiss_long <- gather(avg_ntilde_poiss, training, estimate, 
                                paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model = "BBmixP", N = rep(Ns[1:length(Ns)], each = D))

avg_ntilde_negbin_long <- gather(avg_ntilde_negbin, training, estimate, 
                                 paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model = "BBmixNB", N = rep(Ns[1:length(Ns)], each = D))

joint_ntilde_long <- bind_rows(avg_ntilde_poiss_long, avg_ntilde_negbin_long) %>%
      mutate(Model = fct_relevel(Model, c("BBmixP", "BBmixNB"))) 


# plots
ggplot(joint_ntilde_long, aes( y=estimate, x=Model, fill=Model)) + 
  geom_boxplot() + 
  geom_hline(yintercept = 500,color="black", linetype="dashed", linewidth=0.8) +
  facet_wrap(~N, labeller = labeller(N = ~ paste("n = ", .x)), scales = "free", nrow = 1) +
  theme_bw() + 
  theme(strip.text.x = element_text(size = 12))+
  theme(#panel.grid.major = element_blank(),
        #panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = pretty_breaks()) +
  rremove("xlab") + ylab("# distinct features") +
  scale_fill_manual(values = c("BBmixP" = "forestgreen",
                                  "BBmixNB" = "royalblue1")) 

ggsave(filename = "Plots_paper/plot_m5_rep_rich.png", width = 10, height = 4, dpi = 300, units = "in", device='png')


```






\newpage
## Unbounded-features scenarios
We consider 3 different scenarios corresponding to 3 different growth rates of the number of distinct features observed in the population. Specifically, we are going to consider the following generating mechanisms: set $\pi_k = \frac{1}{k^\xi}$, for $k =1,\ldots,H$, with $H = 10^5$, and $\xi \in \{0.8, 1, 1.2\}$. Note that the higher the value of $\xi$, the slower the growth rate of the number of distinct features is. Note: in the following simulations related to mixtures of Beta-Bernoulli, the $\alpha$ parameter (which belongs to $(-\infty, 0)$) tends to concentrate around $0$, since the model strives to mimic the unbounded-features framework. For this reason, the mixing of such a chain is sometimes not ideal, but we can just put a prior on $\alpha$ (e.g. smaller variance) which makes the mixing better. The produced inference is overall invariant, in the sense that the pictures of rarefaction curves, etc.. look the same. 


### Scenario 1: polynomial growth with exponent 0.8

```{r, include=FALSE, eval=TRUE}
####
##### Unbounded-features scenario ############################
####

rm(list=ls())
library(ggmcmc)
library(coda)
library(ggpubr)
library(scales)

##### Single dataset -> Ntilde (Poiss/NB) and extrapolation (Poiss/NB/Gamma) #####

###### 1) Read results:  MCMC convergence ####################
load(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_params_poiss.Rda")
load(file =  "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_params_negbin.Rda")
load(file =  "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_params_ibp.Rda" )
load(file =  "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_params_sp.Rda" )

###### 2) Read results: samples from limiting distributions (Poiss/NB) ##############
load(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_ntilde_poiss.Rda")
load(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_ntilde_negbin.Rda")

###### 3) Read results: CI for extrapolation (Poiss/NB/Gamma) ################
list_kmn_pred_test_poiss <- readRDS(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_ci_poiss.rds")
list_kmn_pred_test_negbin <- readRDS(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_ci_negbin.rds")
list_kmn_pred_test_ibp <- readRDS(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_ci_ibp.rds")
list_kmn_pred_test_sp <- readRDS(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_ci_sp.rds")


###### 3.b) Read results: CI for insample (Poiss/NB/Gamma) ################
list_kn_rarefaction_poiss <- readRDS(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_ci_insample_poiss.rds")
list_kn_rarefaction_negbin <- readRDS(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_ci_insample_negbin.rds")
list_kn_rarefaction_ibp <- readRDS(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_ci_insample_ibp.rds")
list_kn_rarefaction_sp <- readRDS(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_ci_insample_sp.rds")

###### 4) Read the data ###############################
data_mat <- readRDS(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_data_mat.rds")
data_list <- create_features_list(data_mat)
L <- nrow(data_mat)
Ms <- sapply(list_kmn_pred_test_poiss, function(l) length(l$medians))
Ns <- L - Ms



##### Accuracy on multiple datasets #####

###### 1) Read results: limit distribution estimates #####
load(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_avg_ntilde_poiss.Rda")
load(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_avg_ntilde_negbin.Rda")

###### 2) Read results: quantities on accuracy #####
load(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_obs_train.Rda")
load(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_obs_new.Rda")

load(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_est_new_poiss.Rda")
load(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_est_new_negbin.Rda")
load(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_est_new_ibp.Rda")
load(file = "unbounded_features_simulation/unb_poly_0_8/unb_poly_0_8_est_new_sp.Rda")

D <- nrow(avg_ntilde_poiss)

```

Set $\pi_k = \frac{1}{k^{0.8}}$, for $k =1,\ldots,H$, with $H = 10^5$. Let $L$ be the total dimension of the dataset, and consider different dimensions for the training set $n$, i.e. 

```{r, echo=FALSE}
# Total dimension of the dataset
cat("L = ",L, "\n", sep = "\t")
# Different dimensions of the training set
cat("n = ",as.numeric(Ns), "\n", sep = "\t")
```

Here, the curve representing the number of observed features in increasing samples, where the grey vertical lines indicate the different training dimensions.

```{r, echo=FALSE, fig.align = "center"}

plot_trajectory(data_list) + 
  ggtitle("Number of observed features in increasing samples") +
  geom_vline(xintercept = Ns, color="grey", linetype="dashed", linewidth=1)

```

\newpage
Here, we report (i.a) the in-sample rarefaction curve (on a single dataset).

```{r, echo=FALSE}
gg_kn_rarefaction_all  <- vector(mode="list", length = length(Ns))

for (j in 1:length(Ns)){
  N <- Ns[j]
  
  train_mat <- data_mat[1:N,]
  # convert the binary matrix into list of features
  train_list <- create_features_list(train_mat)
  
  gg_kn_rarefaction_all[[j]] <- plot_Kn_median_and_rarefaction_all_sp(
    train_list = train_list,
    ci_poiss = list_kn_rarefaction_poiss[[paste0("N.",N)]], 
    ci_negbin = list_kn_rarefaction_negbin[[paste0("N.",N)]],
    ci_ibp = list_kn_rarefaction_ibp[[paste0("N.",N)]],
    ci_sp = list_kn_rarefaction_sp[[paste0("N.",N)]],
    n_avg = 100) +
    ggtitle(paste0("n = ", N)) + theme(plot.title = element_text(size=12))
  
  if (j != 1){
    gg_kn_rarefaction_all[[j]] <- gg_kn_rarefaction_all[[j]] +
      theme(#axis.text.y = element_blank(),
            #axis.ticks.y = element_blank(),
            axis.title.y = element_blank() )
  }
  
}
```

```{r, echo=FALSE, fig.align = "center"}
# Print plots
fig_rare_ <- wrap_plots(gg_kn_rarefaction_all, nrow = 1, ncol = 4) +
  plot_layout(guides = "collect") & theme(legend.position = 'right') & xlab(NULL) & theme(plot.margin = margin(5.5, 5.5, 0, 5.5))

fig_rare <- wrap_elements(panel = fig_rare_) +
  labs(tag = "# observations") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom") 

ggsave(filename = "Plots_paper/plot_unb_poly_0_8_rare.png", width = 10, height = 4, dpi = 300, units = "in", device='png')
```

Here, we report (i.b) the extrapolated rarefaction curve (on a single dataset).

```{r, echo=FALSE}
gg_kmn_pred_test_all  <- vector(mode="list", length = length(Ns))

for (j in 1:length(Ns)){
  N <- Ns[j]
  M <- L - N
  
  train_mat <- data_mat[1:N,]
  test_mat <- data_mat[(N+1):L, ]
  # convert the binary matrix into list of features
  train_list <- create_features_list(train_mat)
  test_list <- create_features_list(test_mat)
  
  
  gg_kmn_pred_test_all[[j]] <- plot_Kmn_median_pred_and_test_all_sp(
    train_list = train_list,
    test_list = test_list,
    ci_poiss = list_kmn_pred_test_poiss[[paste0("N.",N)]], 
    ci_negbin = list_kmn_pred_test_negbin[[paste0("N.",N)]],
    ci_ibp = list_kmn_pred_test_ibp[[paste0("N.",N)]],
    ci_sp = list_kmn_pred_test_sp[[paste0("N.",N)]],
    n_avg = 100) +
    ggtitle(paste0("n = ", N)) + theme(plot.title = element_text(size=12))

  
  if (j != 1){
    gg_kmn_pred_test_all[[j]] <- gg_kmn_pred_test_all[[j]] +
      theme(#axis.text.y = element_blank(),
            #axis.ticks.y = element_blank(),
            axis.title.y = element_blank() )
  }
  
}

```


```{r, echo=FALSE, fig.align = "center"}
# Print plots
fig_pred_ <- wrap_plots(gg_kmn_pred_test_all, nrow = 1, ncol = 4) +   plot_layout(guides = "collect") & theme(legend.position = 'right') & xlab(NULL) & theme(plot.margin = margin(5.5, 5.5, 0, 5.5))

fig_pred <- wrap_elements(panel = fig_pred_) +
  labs(tag = "# observations") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom") 

ggsave(filename = "Plots_paper/plot_unb_poly_0_8_pred.png", width = 10, height = 4, dpi = 300, units = "in", device='png')

```

Here, we report (ii) the accuracy of the estimated number of unseen features in the test sample, over $D=50$ replicated datasets. 

```{r, echo=FALSE, fig.align = "center"}
# Print plots
acc_alt_poiss <- compute_accuracy(obs_new, est_new_poiss, obs_train)

acc_alt_negbin <- compute_accuracy(obs_new, est_new_negbin, obs_train)

acc_alt_ibp <- compute_accuracy(obs_new, est_new_ibp, obs_train)

acc_alt_sp <- compute_accuracy(obs_new, est_new_sp, obs_train)


acc_alt_poiss_long <- gather(acc_alt_poiss, training, Accuracy, 
                             paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "BBmixP", N = rep(Ns, each = D))

acc_alt_negbin_long <- gather(acc_alt_negbin, training , Accuracy, 
                              paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "BBmixNB", N = rep(Ns, each = D))

acc_alt_ibp_long <- gather(acc_alt_ibp, training , Accuracy, 
                           paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "3IBPmix", N = rep(Ns, each = D))

acc_alt_sp_long <- gather(acc_alt_sp, training , Accuracy, 
                          paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model = "SB-SP", N = rep(Ns, each = D))

joint_alt_long <- bind_rows(acc_alt_poiss_long, acc_alt_negbin_long, 
                            acc_alt_ibp_long, acc_alt_sp_long) %>%
    mutate(Model = fct_relevel(Model, c("3IBPmix", "SB-SP",                                        "BBmixP", "BBmixNB"))) 


# plots
ggplot(joint_alt_long, aes( y=Accuracy, x=Model, fill=Model)) + 
  geom_boxplot() + 
  facet_wrap(~N, labeller = labeller(N = ~ paste("n = ", .x)), scales = "free", nrow = 1) +
  theme_bw() + 
  theme(strip.text.x = element_text(size = 12))+
  theme(#panel.grid.major = element_blank(),
        #panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = pretty_breaks()) +
  rremove("xlab") +
  scale_fill_manual(values = c("3IBPmix" = "orangered1" ,
                                  "SB-SP" = "orange3",
                                  "BBmixP" = "forestgreen",
                                  "BBmixNB" = "royalblue1")) 

ggsave(filename = "Plots_paper/plot_unb_poly_0_8_accuracy.png", width = 10, height = 4, dpi = 300, units = "in", device='png')

```


\newpage
### Scenario 2: polynomial growth with exponent 1

```{r, include=FALSE, eval=TRUE}
####
##### Unbounded-features scenario ############################
####

rm(list=ls())
library(ggmcmc)
library(coda)
library(ggpubr)
library(scales)

##### Single dataset -> Ntilde (Poiss/NB) and extrapolation (Poiss/NB/Gamma) #####

###### 1) Read results:  MCMC convergence ####################
load(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_params_poiss.Rda")
load(file =  "unbounded_features_simulation/unb_poly_1/unb_poly_1_params_negbin.Rda")
load(file =  "unbounded_features_simulation/unb_poly_1/unb_poly_1_params_ibp.Rda" )
load(file =  "unbounded_features_simulation/unb_poly_1/unb_poly_1_params_sp.Rda" )

###### 2) Read results: samples from limiting distributions (Poiss/NB) ##############
load(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_ntilde_poiss.Rda")
load(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_ntilde_negbin.Rda")

###### 3) Read results: CI for extrapolation (Poiss/NB/Gamma) ################
list_kmn_pred_test_poiss <- readRDS(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_ci_poiss.rds")
list_kmn_pred_test_negbin <- readRDS(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_ci_negbin.rds")
list_kmn_pred_test_ibp <- readRDS(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_ci_ibp.rds")
list_kmn_pred_test_sp <- readRDS(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_ci_sp.rds")

###### 3.b) Read results: CI for insample (Poiss/NB/Gamma) ################
list_kn_rarefaction_poiss <- readRDS(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_ci_insample_poiss.rds")
list_kn_rarefaction_negbin <- readRDS(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_ci_insample_negbin.rds")
list_kn_rarefaction_ibp <- readRDS(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_ci_insample_ibp.rds")
list_kn_rarefaction_sp <- readRDS(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_ci_insample_sp.rds")

###### 4) Read the data ###############################
data_mat <- readRDS(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_data_mat.rds")
data_list <- create_features_list(data_mat)
L <- nrow(data_mat)
Ms <- sapply(list_kmn_pred_test_poiss, function(l) length(l$medians))
Ns <- L - Ms



##### Accuracy on multiple datasets #####

###### 1) Read results: limit distribution estimates #####
load(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_avg_ntilde_poiss.Rda")
load(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_avg_ntilde_negbin.Rda")

###### 2) Read results: quantities on accuracy #####
load(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_obs_train.Rda")
load(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_obs_new.Rda")

load(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_est_new_poiss.Rda")
load(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_est_new_negbin.Rda")
load(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_est_new_ibp.Rda")
load(file = "unbounded_features_simulation/unb_poly_1/unb_poly_1_est_new_sp.Rda")

D <- nrow(avg_ntilde_poiss)

```

Set $\pi_k = \frac{1}{k}$, for $k =1,\ldots,H$, with $H = 10^5$. Let $L$ be the total dimension of the dataset, and consider different dimensions for the training set $n$, i.e. 

```{r, echo=FALSE}
# Total dimension of the dataset
cat("L = ",L, "\n", sep = "\t")
# Different dimensions of the training set
cat("n = ",as.numeric(Ns), "\n", sep = "\t")
```

Here, the curve representing the number of observed features in increasing samples, where the grey vertical lines indicate the different training dimensions.

```{r, echo=FALSE, fig.align = "center"}

plot_trajectory(data_list) + 
  ggtitle("Number of observed features in increasing samples") +
  geom_vline(xintercept = Ns, color="grey", linetype="dashed", linewidth=1)

```

\newpage
Here, we report (i.a) the in-sample rarefaction curve (on a single dataset).

```{r, echo=FALSE}
gg_kn_rarefaction_all  <- vector(mode="list", length = length(Ns))

for (j in 1:length(Ns)){
  N <- Ns[j]
  
  train_mat <- data_mat[1:N,]
  # convert the binary matrix into list of features
  train_list <- create_features_list(train_mat)
  
  gg_kn_rarefaction_all[[j]] <- plot_Kn_median_and_rarefaction_all_sp(
    train_list = train_list,
    ci_poiss = list_kn_rarefaction_poiss[[paste0("N.",N)]], 
    ci_negbin = list_kn_rarefaction_negbin[[paste0("N.",N)]],
    ci_ibp = list_kn_rarefaction_ibp[[paste0("N.",N)]],
    ci_sp = list_kn_rarefaction_sp[[paste0("N.",N)]],
    n_avg = 100) +
    ggtitle(paste0("n = ", N)) + theme(plot.title = element_text(size=12))
  
  if (j != 1){
    gg_kn_rarefaction_all[[j]] <- gg_kn_rarefaction_all[[j]] +
      theme(#axis.text.y = element_blank(),
            #axis.ticks.y = element_blank(),
            axis.title.y = element_blank() )
  }
  
}
```

```{r, echo=FALSE, fig.align = "center"}
# Print plots
fig_rare_ <- wrap_plots(gg_kn_rarefaction_all, nrow = 1, ncol = 4) +
  plot_layout(guides = "collect") & theme(legend.position = 'right') & xlab(NULL) & theme(plot.margin = margin(5.5, 5.5, 0, 5.5))

fig_rare <- wrap_elements(panel = fig_rare_) +
  labs(tag = "# observations") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom") 

ggsave(filename = "Plots_paper/plot_unb_poly_1_rare.png", width = 10, height = 4, dpi = 300, units = "in", device='png')
```

Here, we report (i.b) the extrapolated rarefaction curve (on a single dataset).

```{r, echo=FALSE}
gg_kmn_pred_test_all  <- vector(mode="list", length = length(Ns))

for (j in 1:length(Ns)){
  N <- Ns[j]
  M <- L - N
  
  train_mat <- data_mat[1:N,]
  test_mat <- data_mat[(N+1):L, ]
  # convert the binary matrix into list of features
  train_list <- create_features_list(train_mat)
  test_list <- create_features_list(test_mat)
  
  
  gg_kmn_pred_test_all[[j]] <- plot_Kmn_median_pred_and_test_all_sp(
    train_list = train_list,
    test_list = test_list,
    ci_poiss = list_kmn_pred_test_poiss[[paste0("N.",N)]], 
    ci_negbin = list_kmn_pred_test_negbin[[paste0("N.",N)]],
    ci_ibp = list_kmn_pred_test_ibp[[paste0("N.",N)]],
    ci_sp = list_kmn_pred_test_sp[[paste0("N.",N)]],
    n_avg = 100) +
    ggtitle(paste0("n = ", N)) + theme(plot.title = element_text(size=12))

  
  if (j != 1){
    gg_kmn_pred_test_all[[j]] <- gg_kmn_pred_test_all[[j]] +
      theme(#axis.text.y = element_blank(),
            #axis.ticks.y = element_blank(),
            axis.title.y = element_blank() )
  }
  
}

```


```{r, echo=FALSE, fig.align = "center"}
# Print plots
fig_pred_ <- wrap_plots(gg_kmn_pred_test_all, nrow = 1, ncol = 4) +   plot_layout(guides = "collect") & theme(legend.position = 'right') & xlab(NULL) & theme(plot.margin = margin(5.5, 5.5, 0, 5.5))

fig_pred <- wrap_elements(panel = fig_pred_) +
  labs(tag = "# observations") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom") 

ggsave(filename = "Plots_paper/plot_unb_poly_1_pred.png", width = 10, height = 4, dpi = 300, units = "in", device='png')

```
Here, we report (ii) the accuracy of the estimated number of unseen features in the test sample, over $D=50$ replicated datasets. 

```{r, echo=FALSE, fig.align = "center"}
# Print plots
acc_alt_poiss <- compute_accuracy(obs_new, est_new_poiss, obs_train)

acc_alt_negbin <- compute_accuracy(obs_new, est_new_negbin, obs_train)

acc_alt_ibp <- compute_accuracy(obs_new, est_new_ibp, obs_train)

acc_alt_sp <- compute_accuracy(obs_new, est_new_sp, obs_train)


acc_alt_poiss_long <- gather(acc_alt_poiss, training, Accuracy, 
                             paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "BBmixP", N = rep(Ns, each = D))

acc_alt_negbin_long <- gather(acc_alt_negbin, training , Accuracy, 
                              paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "BBmixNB", N = rep(Ns, each = D))

acc_alt_ibp_long <- gather(acc_alt_ibp, training , Accuracy, 
                           paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "3IBPmix", N = rep(Ns, each = D))

acc_alt_sp_long <- gather(acc_alt_sp, training , Accuracy, 
                          paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model = "SB-SP", N = rep(Ns, each = D))

joint_alt_long <- bind_rows(acc_alt_poiss_long, acc_alt_negbin_long, 
                            acc_alt_ibp_long, acc_alt_sp_long) %>%
    mutate(Model = fct_relevel(Model, c("3IBPmix", "SB-SP",                                        "BBmixP", "BBmixNB"))) 


# plots
ggplot(joint_alt_long, aes( y=Accuracy, x=Model, fill=Model)) + 
  geom_boxplot() + 
  facet_wrap(~N, labeller = labeller(N = ~ paste("n = ", .x)), scales = "free", nrow = 1) +
  theme_bw() +
  theme(strip.text.x = element_text(size = 12))+
  theme(#panel.grid.major = element_blank(),
        #panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = pretty_breaks()) +
  rremove("xlab") +
  scale_fill_manual(values = c("3IBPmix" = "orangered1" ,
                                  "SB-SP" = "orange3",
                                  "BBmixP" = "forestgreen",
                                  "BBmixNB" = "royalblue1")) 

ggsave(filename = "Plots_paper/plot_unb_poly_1_accuracy.png", width = 10, height = 4, dpi = 300, units = "in", device='png')

```



\newpage
### Scenario 3: polynomial growth with exponent 1.2

```{r, include=FALSE, eval=TRUE}
####
##### Unbounded-features scenario ############################
####

rm(list=ls())
library(ggmcmc)
library(coda)
library(ggpubr)
library(scales)

##### Single dataset -> Ntilde (Poiss/NB) and extrapolation (Poiss/NB/Gamma) #####

###### 1) Read results:  MCMC convergence ####################
load(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_params_poiss.Rda")
load(file =  "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_params_negbin.Rda")
load(file =  "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_params_ibp.Rda" )
load(file =  "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_params_sp.Rda" )

###### 2) Read results: samples from limiting distributions (Poiss/NB) ##############
load(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_ntilde_poiss.Rda")
load(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_ntilde_negbin.Rda")

###### 3) Read results: CI for extrapolation (Poiss/NB/Gamma) ################
list_kmn_pred_test_poiss <- readRDS(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_ci_poiss.rds")
list_kmn_pred_test_negbin <- readRDS(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_ci_negbin.rds")
list_kmn_pred_test_ibp <- readRDS(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_ci_ibp.rds")
list_kmn_pred_test_sp <- readRDS(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_ci_sp.rds")

###### 3.b) Read results: CI for insample (Poiss/NB/Gamma) ################
list_kn_rarefaction_poiss <- readRDS(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_ci_insample_poiss.rds")
list_kn_rarefaction_negbin <- readRDS(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_ci_insample_negbin.rds")
list_kn_rarefaction_ibp <- readRDS(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_ci_insample_ibp.rds")
list_kn_rarefaction_sp <- readRDS(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_ci_insample_sp.rds")

###### 4) Read the data ###############################
data_mat <- readRDS(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_data_mat.rds")
data_list <- create_features_list(data_mat)
L <- nrow(data_mat)
Ms <- sapply(list_kmn_pred_test_poiss, function(l) length(l$medians))
Ns <- L - Ms



##### Accuracy on multiple datasets #####

###### 1) Read results: limit distribution estimates #####
load(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_avg_ntilde_poiss.Rda")
load(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_avg_ntilde_negbin.Rda")

###### 2) Read results: quantities on accuracy #####
load(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_obs_train.Rda")
load(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_obs_new.Rda")

load(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_est_new_poiss.Rda")
load(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_est_new_negbin.Rda")
load(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_est_new_ibp.Rda")
load(file = "unbounded_features_simulation/unb_poly_1_2/unb_poly_1_2_est_new_sp.Rda")

D <- nrow(avg_ntilde_poiss)

```

Set $\pi_k = \frac{1}{k^{1.2}}$, for $k =1,\ldots,H$, with $H = 10^5$. Let $L$ be the total dimension of the dataset, and consider different dimensions for the training set $n$, i.e. 

```{r, echo=FALSE}
# Total dimension of the dataset
cat("L = ",L, "\n", sep = "\t")
# Different dimensions of the training set
cat("n = ",as.numeric(Ns), "\n", sep = "\t")
```

Here, the curve representing the number of observed features in increasing samples, where the grey vertical lines indicate the different training dimensions.

```{r, echo=FALSE, fig.align = "center"}

plot_trajectory(data_list) + 
  ggtitle("Number of observed features in increasing samples") +
  geom_vline(xintercept = Ns, color="grey", linetype="dashed", linewidth=1)

```

\newpage
Here, we report (i.a) the in-sample rarefaction curve (on a single dataset).

```{r, echo=FALSE}
gg_kn_rarefaction_all  <- vector(mode="list", length = length(Ns))

for (j in 1:length(Ns)){
  N <- Ns[j]
  
  train_mat <- data_mat[1:N,]
  # convert the binary matrix into list of features
  train_list <- create_features_list(train_mat)
  
  gg_kn_rarefaction_all[[j]] <- plot_Kn_median_and_rarefaction_all_sp(
    train_list = train_list,
    ci_poiss = list_kn_rarefaction_poiss[[paste0("N.",N)]], 
    ci_negbin = list_kn_rarefaction_negbin[[paste0("N.",N)]],
    ci_ibp = list_kn_rarefaction_ibp[[paste0("N.",N)]],
    ci_sp = list_kn_rarefaction_sp[[paste0("N.",N)]],
    n_avg = 100) +
    ggtitle(paste0("n = ", N)) + theme(plot.title = element_text(size=12))
  
  if (j != 1){
    gg_kn_rarefaction_all[[j]] <- gg_kn_rarefaction_all[[j]] +
      theme(#axis.text.y = element_blank(),
            #axis.ticks.y = element_blank(),
            axis.title.y = element_blank() )
  }
  
}
```

```{r, echo=FALSE, fig.align = "center"}
# Print plots
fig_rare_ <- wrap_plots(gg_kn_rarefaction_all, nrow = 1, ncol = 4) +
  plot_layout(guides = "collect") & theme(legend.position = 'right') & xlab(NULL) & theme(plot.margin = margin(5.5, 5.5, 0, 5.5))

fig_rare <- wrap_elements(panel = fig_rare_) +
  labs(tag = "# observations") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom") 

ggsave(filename = "Plots_paper/plot_unb_poly_1_2_rare.png", width = 10, height = 4, dpi = 300, units = "in", device='png')
```


Here, we report (i.b) the extrapolated rarefaction curve (on a single dataset).

```{r, echo=FALSE}
gg_kmn_pred_test_all  <- vector(mode="list", length = length(Ns))

for (j in 1:length(Ns)){
  N <- Ns[j]
  M <- L - N
  
  train_mat <- data_mat[1:N,]
  test_mat <- data_mat[(N+1):L, ]
  # convert the binary matrix into list of features
  train_list <- create_features_list(train_mat)
  test_list <- create_features_list(test_mat)
  
  
  gg_kmn_pred_test_all[[j]] <- plot_Kmn_median_pred_and_test_all_sp(
    train_list = train_list,
    test_list = test_list,
    ci_poiss = list_kmn_pred_test_poiss[[paste0("N.",N)]], 
    ci_negbin = list_kmn_pred_test_negbin[[paste0("N.",N)]],
    ci_ibp = list_kmn_pred_test_ibp[[paste0("N.",N)]],
    ci_sp = list_kmn_pred_test_sp[[paste0("N.",N)]],
    n_avg = 100) +
    ggtitle(paste0("n = ", N)) + theme(plot.title = element_text(size=12))

  
  if (j != 1){
    gg_kmn_pred_test_all[[j]] <- gg_kmn_pred_test_all[[j]] +
      theme(#axis.text.y = element_blank(),
            #axis.ticks.y = element_blank(),
            axis.title.y = element_blank() )
  }
  
}

```


```{r, echo=FALSE, fig.align = "center"}
# Print plots
fig_pred_ <- wrap_plots(gg_kmn_pred_test_all, nrow = 1, ncol = 4) +   plot_layout(guides = "collect") & theme(legend.position = 'right') & xlab(NULL) & theme(plot.margin = margin(5.5, 5.5, 0, 5.5))

fig_pred <- wrap_elements(panel = fig_pred_) +
  labs(tag = "# observations") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom") 

ggsave(filename = "Plots_paper/plot_unb_poly_1_2_pred.png", width = 10, height = 4, dpi = 300, units = "in", device='png')

```
Here, we report (ii) the accuracy of the estimated number of unseen features in the test sample, over $D=50$ replicated datasets. 

```{r, echo=FALSE, fig.align = "center"}
# Print plots
acc_alt_poiss <- compute_accuracy(obs_new, est_new_poiss, obs_train)

acc_alt_negbin <- compute_accuracy(obs_new, est_new_negbin, obs_train)

acc_alt_ibp <- compute_accuracy(obs_new, est_new_ibp, obs_train)

acc_alt_sp <- compute_accuracy(obs_new, est_new_sp, obs_train)


acc_alt_poiss_long <- gather(acc_alt_poiss, training, Accuracy, 
                             paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "BBmixP", N = rep(Ns, each = D))

acc_alt_negbin_long <- gather(acc_alt_negbin, training , Accuracy, 
                              paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "BBmixNB", N = rep(Ns, each = D))

acc_alt_ibp_long <- gather(acc_alt_ibp, training , Accuracy, 
                           paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model= "3IBPmix", N = rep(Ns, each = D))

acc_alt_sp_long <- gather(acc_alt_sp, training , Accuracy, 
                          paste0("N.",Ns[1]):paste0("N.",Ns[length(Ns)]), factor_key=TRUE) %>%
  add_column(Model = "SB-SP", N = rep(Ns, each = D))

joint_alt_long <- bind_rows(acc_alt_poiss_long, acc_alt_negbin_long, 
                            acc_alt_ibp_long, acc_alt_sp_long) %>%
    mutate(Model = fct_relevel(Model, c("3IBPmix", "SB-SP",                                        "BBmixP", "BBmixNB"))) 


# plots
ggplot(joint_alt_long, aes( y=Accuracy, x=Model, fill=Model)) + 
  geom_boxplot() + 
  facet_wrap(~N, labeller = labeller(N = ~ paste("n = ", .x)), scales = "free", nrow = 1) +
  theme_bw() + 
  theme(strip.text.x = element_text(size = 12))+
  theme(#panel.grid.major = element_blank(),
        #panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = pretty_breaks()) +
  rremove("xlab") +
  scale_fill_manual(values = c("3IBPmix" = "orangered1" ,
                                  "SB-SP" = "orange3",
                                  "BBmixP" = "forestgreen",
                                  "BBmixNB" = "royalblue1")) 

ggsave(filename = "Plots_paper/plot_unb_poly_1_2_accuracy.png", width = 10, height = 4, dpi = 300, units = "in", device='png')

```
